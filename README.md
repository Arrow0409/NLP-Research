Generating and Comparing Adversarial Attacks on Text-Based Sentiment Detector using Integrated Gradient
This project investigates the vulnerabilities of text-based sentiment detectors to adversarial manipulations and evaluates various attack techniques to enhance their robustness. Our primary focus is on leveraging Integrated Gradients (IG) as a novel approach to generate more effective defenses against adversarial perturbations.
Key Features:
Implementation of Adversarial Attacks: We implement and compare the performance of various adversarial attack algorithms, including Fast Gradient Sign Method (FGSM), K-Projected Gradient Descent (K-PGD), and Integrated Gradients (IG).
Model Robustness Evaluation: We analyze the impact of these attacks on the model's accuracy, loss, hits, misses, elapsed time, and precision.
Comparative Analysis: We compare the effectiveness of different attack techniques to understand their advantages and limitations.
Integrated Gradients: We demonstrate the potential of IG as a promising technique for enhancing model robustness while maintaining computational efficiency.
Project Goals:
Develop robust text-based sentiment analysis models: We aim to create models that can withstand adversarial manipulations and maintain their accuracy and reliability.
Understand adversarial attack mechanisms: We aim to gain deeper insights into how adversarial attacks manipulate model behavior and lead to misinterpretations.
Contribute to the development of more secure and trustworthy AI systems: We strive to contribute to the ongoing efforts to develop more robust and trustworthy AI systems.

Dataset: https://www.tensorflow.org/datasets/catalog/imdb_reviews
