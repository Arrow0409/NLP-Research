# NLP-Research
This research delves into the realm of Text-based Sentiment Detectors, specifically focusing on the generation of adversarial attacks using Integrated Gradients, and also seeks to build the robustness of Neural Networks against adversarial attacks. Integrated Gradients, a feature attribution method, illuminate the influence of each input feature on the modelâ€™s output, thereby crafting subtle and potent adversarial examples. The study aims to investigate the vulnerabilities of these sentiment detectors to adversarial manipulations and enhance our understanding of the robustness and weaknesses inherent in such systems mainly to make it more secure and robust. We compare this approach with the Fast Gradient Sign Method (FGSM), a well-known technique for its computational efficiency in generating adversarial noise. Our study aims to evaluate the effectiveness of adversarial examples produced by Integrated Gradients against those from FGSM, in terms of their ability to deceive sentiment detection models while maintaining semantic coherence and imperceptibility to human readers. Through extensive experiments, we establish a framework for assessing the resilience of sentiment detectors and propose a novel metric for comparing the adversarial robustness of different models. The outcomes of this research will not only contribute to the understanding of adversarial attacks in the context of sentiment analysis but also aid in the development of more secure and reliable NLP systems. This abstract encapsulates the essence of the project, highlighting the comparison between Integrated Gradients and FGSM in the context of adversarial attacks on sentiment detectors.
